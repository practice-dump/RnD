This contains learning I gather from [Language models are few short learners](https://arxiv.org/pdf/2005.14165.pdf)  aka GPT3 paper which shouldn't be a google search away

Found an [archieved Git](https://github.com/openai/gpt-3) it contains some stats and data files which might be beneficial 

To read after this

- [ ] GPT2 paper for architecture
- [ ] Sparse transformer paper to understand attention paper
- [ ] [gradient noise scale to calculate batch size](https://arxiv.org/pdf/1812.06162.pdf)
